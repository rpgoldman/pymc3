# Stubs for pymc3.variational.updates (Python 3.7)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from typing import Any, Optional

def sgd(loss_or_grads: Optional[Any] = ..., params: Optional[Any] = ..., learning_rate: float = ...): ...
def apply_momentum(updates: Any, params: Optional[Any] = ..., momentum: float = ...): ...
def momentum(loss_or_grads: Optional[Any] = ..., params: Optional[Any] = ..., learning_rate: float = ..., momentum: float = ...): ...
def apply_nesterov_momentum(updates: Any, params: Optional[Any] = ..., momentum: float = ...): ...
def nesterov_momentum(loss_or_grads: Optional[Any] = ..., params: Optional[Any] = ..., learning_rate: float = ..., momentum: float = ...): ...
def adagrad(loss_or_grads: Optional[Any] = ..., params: Optional[Any] = ..., learning_rate: float = ..., epsilon: float = ...): ...
def adagrad_window(loss_or_grads: Optional[Any] = ..., params: Optional[Any] = ..., learning_rate: float = ..., epsilon: float = ..., n_win: int = ...): ...
def rmsprop(loss_or_grads: Optional[Any] = ..., params: Optional[Any] = ..., learning_rate: float = ..., rho: float = ..., epsilon: float = ...): ...
def adadelta(loss_or_grads: Optional[Any] = ..., params: Optional[Any] = ..., learning_rate: float = ..., rho: float = ..., epsilon: float = ...): ...
def adam(loss_or_grads: Optional[Any] = ..., params: Optional[Any] = ..., learning_rate: float = ..., beta1: float = ..., beta2: float = ..., epsilon: float = ...): ...
def adamax(loss_or_grads: Optional[Any] = ..., params: Optional[Any] = ..., learning_rate: float = ..., beta1: float = ..., beta2: float = ..., epsilon: float = ...): ...
def norm_constraint(tensor_var: Any, max_norm: Any, norm_axes: Optional[Any] = ..., epsilon: float = ...): ...
def total_norm_constraint(tensor_vars: Any, max_norm: Any, epsilon: float = ..., return_norm: bool = ...): ...
